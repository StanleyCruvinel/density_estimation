---
title: "Comments"
author: "Tom√°s Capretto"
date: "27/3/2020"
output: html_document
---

## Time

### Fixed Gaussian

The computation times are not related to the density that is being estimated. The comments/conclusions presented apply to each method in general.

The best times are observed when the bandwidth is estimated via Scott's or Silverman's rule. Their computational times are identical, irrespectively of sample size. The mean computation time ranges from 0.7ms when $n=200$ to 1.1ms when $n=10000$.

Both Scott's and Silverman's rules have identical computational times because they perform the same amount of operations (they just differ in a multiplicative constant).

The Improved Sheather-Jones is between 6 and 7 times slower than the rules of thumb, but its demand is constant across the different sample sizes. Consequently, for a sample size of 10000 it is ~ 3.5 times slower than Scott's and Silverman's rule.

The experimental bandwidth, which requires computing both a rule of thumb and the improved Sheather-Jones bandwidths, has a time demand approximately equal to the sum of both. It does not imply a substantial overhead compared to ISJ.

The Sheather-Jones method is not suitable for a fast and practical usage. Even with a size of 1000 the method takes more than 2 seconds on average. 

### Adaptive Gaussian

The adaptive estimator is between 1.5 and 3.5 times slower than the Fixed Gaussian KDE (see any of the Gaussian/Gaussian mixtures heatmaps).

If we consider the heatmaps related to the Gamma, Beta and LogNormal distributions we will see computational times multiplied between 15 and 50 times. The increase is not only due to the adaptivity. It also represents the overhead induced by the boundary reflection method. 

Whe the boundary correction is applied with a fixed Gaussian KDE via convolution, this does not induce a substantial overhead. But when it is applied with an adaptive KDE, the overhead is important because you cannot use a fast method to perform the calculatons.

----------------------------------------

Idea: The estimated computation times could be used to flag users, or at least to be included in documentation.

The ISJ method could be further improved. Within the method you compute a relative frequency per bin. These could be re-utilized in the FFT. Currently they are being computed twice. What's more, other things (like minimum, maximum, SDs, etc) are being computed twice.