{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Problem statement\n",
    "\n",
    "Recall the Asymptotic Mean Integrated Squared Error and the bandwidth $h$ that minimizes it: \n",
    "\n",
    "$$\n",
    "\\text{AMISE} = \\frac{1}{4} h^4 R(f'') + \\frac{1}{2N\\sqrt{\\pi}h}\n",
    "\\tag{3.1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_{\\text{AMISE}} = \\left(\\frac{1}{2N\\sqrt{\\pi} R(f'')} \\right)^{1 / 5}\n",
    "\\tag{3.2}\n",
    "$$\n",
    "\n",
    "The **Gaussian rule of thumb** to estimate $h$ is derived under the assumption that $f$ is a Gaussian density. Despite the criticism at a conceptual level it is indeed a useful rule in many practical situations. On the other hand, its simplicity does not come at a free cost. When the true $f$ differs seriously from a Gaussian distribution, the Gaussian KDE with bandwidth estimated by either Scotts' or Silverman's rule results in a poor alternative.\n",
    "\n",
    "It can be seen the optimal $h$ depends on $R(f'') = \\int{[f''(x)]^2 dx}$, which is a measure of how wiggly $f$ is. AMISE formula tells that higher values of $R(f'')$ result in higher bias. In other words, more sinuous densities are more difficult to estimate.\n",
    "\n",
    "The more $f$ differs from a normal distribution in terms of roughness, the worse the gaussian rule of thumb is. If the density we are trying to estimate has peaks and/or multiple modes, the estimation will oversmooth the true distribution. Particularly, the problem is that we are replacing the true $R(f'')$ with $R(g_{\\text{gaussian}}'')$ when actually $R(f'') \\gg R(g_{\\text{gaussian}}'')$.\n",
    "\n",
    "Im the paper *A brief survey on bandwidth selection for density estimation*, Jones, Marron and Sheather (1996) studied the Monte Carlo performance of the normal reference bandwidth based on the \n",
    "standard deviation, that is, they considered $h = 1.06\\hat{\\sigma}N^{-0.2}$. \n",
    "They found that $h$ had a mean that was usually unacceptably large and thus often produced oversmoother density estimates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from my_functions import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 7]\n",
    "\n",
    "BLUE = \"#3498db\"\n",
    "DARK_BLUE = \"#2980b9\"\n",
    "NIGHT_BLUE = \"#2c3e50\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code generates 50 samples of size 3000 from a mixture of Gaussian distributions with variable mean and standard deviation. It can be seen that while the estimator has low variance, it has high bias.  \n",
    "The density to be estimated is more sinuous than a Gaussian, and consequently, the estimator oversmooths the true $f$. We can appreciate the estimator puts more the density when it is actually very low (i.e. in the left tail) and it fails to detect the two modes on the left part of the plot.\n",
    "\n",
    "For some applications, the result could still be good enough, as we can see the general pattern of the underlying distribution. For other applications where it is desired to obtain an accurate estimation of the modes, the result is unsastifactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rvs(n, mean, sd):\n",
    "    assert len(mean) == len(sd)\n",
    "    x = np.concatenate((\n",
    "        list(map(lambda m, s: stats.norm.rvs(m, s, int(n / len(mean))), mean, sd))\n",
    "    ))\n",
    "    return x\n",
    "\n",
    "def get_pdf(x_grid, mean, sd):\n",
    "    assert len(mean) == len(sd)\n",
    "    pdf = (1 / len(mean)) * np.sum(list((map(lambda m, s: stats.norm.pdf(x_grid, m, s), mean, sd))), axis=0)\n",
    "    return pdf\n",
    "\n",
    "np.random.seed(1234)\n",
    "x_grid = np.linspace(-6, 7, 500)\n",
    "mean = [-3, -1.8, 3]\n",
    "sd = [0.3, 0.45, 2]\n",
    "n = 3000\n",
    "reps = 50\n",
    "\n",
    "colors = [DARK_BLUE, NIGHT_BLUE]\n",
    "lines = [Line2D([0], [0], color=c, linewidth=4) for c in colors]\n",
    "labels = [\"True distribution\", \"Estimation\"]\n",
    "\n",
    "pdf_true = get_pdf(x_grid, mean, sd)\n",
    "plt.plot(x_grid, pdf_true, linewidth=4, color=DARK_BLUE)\n",
    "\n",
    "for i in range(reps):\n",
    "    rvs = generate_rvs(n, mean, sd)\n",
    "    x_kde, y_kde = convolution_kde(rvs)\n",
    "    plt.plot(x_kde, y_kde, linewidth=1, color=NIGHT_BLUE, alpha=0.3)\n",
    "\n",
    "plt.hist(rvs, density=True, bins=40, alpha=0.7, Color=BLUE)\n",
    "plt.xlim(-6, 9)\n",
    "\n",
    "plt.legend(lines, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Alternative bandwidth selectors\n",
    "\n",
    "Many alternative bandwidth selectors for the Gaussian KDE can be found in the literature. An outdated collection of such methods is *A brief survey on bandwidth selection for density estimation* (Jones, Marron and Sheather, 1996). I am not still aware of a similar, but updated, survey.\n",
    "\n",
    "It is possible to classify bandwidth selection methods in (at least) three groups:\n",
    "\n",
    "1. Rules of thumb\n",
    "1. Least Squares Cross-Validation\n",
    "1. Plug-in Approach\n",
    "\n",
    "### 1. Rules of thumb\n",
    "\n",
    "This is the group where Scotts' and Silverman's rules of thumb fall. The basic idea is to replace the unknown part of $h_{AMISE}$, $R(f'')$, by an estimated value based on a parametric family. \n",
    "\n",
    "While location is not important when choosing a kernel, the scale parameter is. Because of this and its simplicity is that a Gaussian distribution with mean 0 and standard deviation $\\sigma$ is chosen.\n",
    "\n",
    "### 2. Cross-Validation\n",
    "\n",
    "#### a. Least Squares Cross-Validation\n",
    "\n",
    "The idea first appeared in *Empirical Choice of Histograms and Kernel density Estimators* (Rudemo, 1982) and *An Alternative Method of Cross-Validation for the Smoothing of Density Estimate* (Bowman, 1984).\n",
    "\n",
    "It is possible to measure the closeness of $\\hat{f}$ and $f$ for a given sample with the Integrated Squared Error (ISE).\n",
    "\n",
    "$$\n",
    "\\text{ISE}(\\hat{f}(x; h)) = \\int{[\\hat{f}(x; h) - f(x)]^2}dx\n",
    "\\tag{3.3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  \\begin{split}\n",
    "    \\text{ISE}(\\hat{f}(x; h)) & = \\int{[\\hat{f}(x; h) - f(x)]^2}dx \\\\\n",
    "     & = \\int{[\\hat{f}(x; h)]^2}dx - 2 \\int{[\\hat{f}(x; h)]^2f(x)}dx + \\int{f^2(x)dx}\n",
    "  \\end{split}\n",
    "  \\tag{3.4}\n",
    "$$\n",
    "\n",
    "It can be noticed the last term of the expansion in (3.4) does not involve $h$. Bowman (1984) proposed choosing the bandwidth as the value of $h$ that minimizes the estiamte of the two other terms\n",
    "\n",
    "$$\n",
    "\\text{LSCV}(h) = \\frac{1}{N} \\sum_{i=1}^N{\\int[\\hat{f}_{-i}(x)]^2 dx} - \\frac{2}{N} \\sum_{i=1}^N{\\hat{f}_{-i}(X_i)}\n",
    "\\tag{3.5}\n",
    "$$\n",
    "\n",
    "where $\\hat{f}_{-i}(x)$ denotes the kernel estimator constructed from the data without the observation $X_i$. The method is usually referred to as least squares cross-validation, since it is based on the leave-one-out density estimator $\\hat{f}_{-i}(x)$.  \n",
    "\n",
    "In *Large sample optimality of least-squares cross-validation in density estimation* (Hall, 1983) it was shown that\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{i=1}^N{\\int[\\hat{f}_{-i}(x)]^2 dx} = \\int{[\\hat{f}(x; h)]^2dx} + O_p\\left(\\frac{1}{n^2h}\\right)\n",
    "\\tag{3.6}\n",
    "$$\n",
    "\n",
    "Hence, because of simpler computation, the least squares cross-validation changed its criterion from (3.5) to\n",
    "\n",
    "$$\n",
    "\\text{LSCV}(h) = \\int{[\\hat{f}(x; h)]^2dx} - \\frac{2}{N} \\sum_{i=1}^N{\\hat{f}_{-i}(X_i)}\n",
    "\\tag{3.7}\n",
    "$$\n",
    "\n",
    "The value of $h$ that minimizes $\\text{LSCV}(h)$ is detoned by $h_{\\text{LSCV}}$.\n",
    "\n",
    "**What is the problem with** $h_{\\text{LSCV}}$ **?**\n",
    "\n",
    "* The least squares cross-validation function can have more than one local minimum. In practice, it is recommended to plot the values of $h$ against $\\text{LSCV}(h)$ instead of just using a minimization program. Some advice in the literature says to use the largest local minimizer of $\\text{LSCV}(h)$ because it gave better empirical results than the global minimizer.\n",
    "* The rate of convergence of $h_{\\text{LSCV}}$ has a rate of convergence of $n^{-1/10}$. Hence, $h_{\\text{LSCV}}$ presents very high variability in practice.\n",
    "\n",
    "The first point makes least squares cross-validation not valid for an automatic usage. The second point is also connected to the fact that the method usually result in very undersmoothed estimations, as opposite to the gaussian rule of thumb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://github.com/JTsolon/least-squares-cross-validation-in-KDE\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy.optimize import minimize\n",
    "from scipy.integrate import quad\n",
    "\n",
    "def get_ise_loocv(h, x, x_min, x_max):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the Integrated Squared Error (ISE) via Leave-One-Out Cross-Validation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 1-D array-like\n",
    "        1 dimensional array of sample data from the variable for which a \n",
    "        density estimate is desired.\n",
    "    h : float\n",
    "        Bandwidth (standard deviation of each Gaussian component)\n",
    "    x_min : float\n",
    "        Lower limit for the domain of the variable\n",
    "    x_max : float\n",
    "        Upper limit for the domain of the variable\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    lscv_error : Float, estimation of the Least Squares Cross-Validation Error.   \n",
    "    \"\"\"\n",
    "    \n",
    "    x_len = len(x)\n",
    "    \n",
    "    dens = sm.nonparametric.KDEUnivariate(x)\n",
    "    dens.fit(kernel='gau', bw=h)\n",
    "    f_squared = lambda x : dens.evaluate(x) ** 2\n",
    "    \n",
    "    # Compute first term of LSCV(h) -- twice the area under `f_squared`\n",
    "    f_sq_twice_area = 2 * quad(f_squared, x_min, x_max)[0]\n",
    "    \n",
    "    # Compute second term of LSCV(h)\n",
    "    f_loocv_sum = 0\n",
    "    for i in range(x_len):\n",
    "        dens1 = sm.nonparametric.KDEUnivariate(np.delete(x, i))\n",
    "        dens1.fit(kernel='gau', bw=h)\n",
    "        f_loocv_sum += dens.evaluate(x[i])\n",
    "    f_loocv_sum *= (2 / x_len)\n",
    "\n",
    "    # LSCV(h)\n",
    "    lscv_error = np.abs(f_sq_twice_area - f_loocv_sum) \n",
    "    \n",
    "    return lscv_error\n",
    "\n",
    "def h_cv(x):\n",
    "    \"\"\"\n",
    "    Computes Least Squares Cross-Validation bandwidth for Gaussian KDE\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 1-D array-like\n",
    "        1 dimensional array of sample data from the variable for which a \n",
    "        density estimate is desired.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    h : float\n",
    "        Bandwidth estimated via Least Squares Cross-Validation\n",
    "    \"\"\"\n",
    "    \n",
    "    x_len = len(x)\n",
    "    x_std = np.std(x)\n",
    "    x_min = np.min(x) - 0.5 * x_std\n",
    "    x_max = np.max(x) + 0.5 * x_std   \n",
    "    \n",
    "    # Silverman's rule as initial value for h\n",
    "    h0 = 0.9 * x_std * x_len ** (-0.2)\n",
    "    \n",
    "    # h is constrained to be larger than 10**(-8)\n",
    "    constraint = ({'type':'ineq', 'fun':lambda x : x - 10 ** (-8)})\n",
    "    result = minimize(get_ise_loocv, h0, args=(x, x_min, x_max), constraints=constraint)\n",
    "    h = result.x\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "colors = [DARK_BLUE, NIGHT_BLUE, \"black\"]\n",
    "labels = [\"True distribution\", r\"Gaussian KDE with $h_{\\mathrm{LSCV}}$\", \"Gaussian KDE with $h_{\\mathrm{Silverman}}$\"]\n",
    "lines = [Line2D([0], [0], color=c, linewidth=4) for c in colors]\n",
    "\n",
    "x_grid = np.linspace(-3.5, 3.5, 500)\n",
    "plt.plot(x_grid, stats.norm.pdf(x_grid), linewidth=4, color=DARK_BLUE)\n",
    "\n",
    "for i in range(20):\n",
    "    rvs = np.random.normal(size=1000)\n",
    "    h_lscv = h_cv(rvs)\n",
    "    x_kde, y_kde = convolution_kde(rvs, h=h_lscv)\n",
    "    plt.plot(x_kde, y_kde, linewidth=2, color=NIGHT_BLUE, alpha=0.3)\n",
    "\n",
    "x_kde, y_kde = convolution_kde(rvs)\n",
    "plt.plot(x_kde, y_kde, linewidth=3, linestyle=\"--\", color=\"black\")   \n",
    "plt.legend(lines, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a sample size like the used in the example one would expect any plausible estimator to approximate the true $f$ very well. The estimator based on $h_{\\text{LSCV}}$ is not only very biased and highly variable, but also very slow. It is not worth it to measure computational times given that it is in the order of the seconds even with a sample size of 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Biased Cross-Validation\n",
    "\n",
    "This method was proposed as an alternative to LSCV. It looks for the $h$ that minimizes an estimate of the AMISE instead of the ISE. \n",
    "\n",
    "This estimator receives the name of *biased* cross-validation in opposition to least squares cross-validation, which is also known as *unbiased* cross-validation because $E[\\text{LSCV}(h)] = \\text{MISE} - \\int{f^2(x)dx}$.\n",
    "\n",
    "Even though the estimator based on BCV, $h_{\\text{BCV}}$ is considerably less variable than $h_{\\text{LSCV}}$, it still faces limitations similar to LSCV. For example, it is recommended not to choose the global minimizer of the objective function and use an alternative that is based on a local minima.\n",
    "\n",
    "Nevertheless, its practical performance has not been good enough to consider it as a plausible alternative to replace the usual combination of Gaussian KDE + Gaussian rule of thumb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Plug-in Methods\n",
    "\n",
    "The drawbacks in LSCV and BCV motivated researchers to look for methods with better convergence rates. The plug-in approach is consists of replacing the unknown quantity $R(f'')$ in the $h_{AMISE}$ expression with an estimate. The difference between this method and the rule of thumb is that the rule of thumb imposes a distribution from a parametric family, while this approach estimates $R(f'')$ from the data using a non-parametric approach.\n",
    "\n",
    "Estimating $R(f'')$ by $R(\\hat{f''_g})$ requires that we choose the $g$ bandwidth for so called pilot estimate, which can be done in many ways. One of the first alternatives was proposed by Park and Marron in *Comparison of data-driven bandwidth selectors* (1990). A year later, Sheather and Jones published *A reliable data-based bandwidth seleciton method for kernel density estimation* where they present a new method that improves the one by Park and Marron.\n",
    "\n",
    "#### a. Sheather-Jones plug-in method\n",
    "\n",
    "This approach is based on writing $g$ for the estimate $R(\\hat{f}'')$ as a function of h\n",
    "\n",
    "$$\n",
    "g(h) = C(\\kappa)\\left[ \\frac{R(f'')}{R(f''')} \\right]^{1/7} h^{5/7}\n",
    "\\tag{3.8}\n",
    "$$\n",
    "\n",
    "and estimating the resulting unknown funcitonals of $f$ using kernel density estimates with bandwidths chosen by normal rules of thumb. In this context, the only unknown quantity in the following expression is $h$,\n",
    "\n",
    "$$\n",
    "\\left[\\frac{R(\\kappa)}{\\mu_2(\\kappa)^2\\hat{S}(g(h))} \\right] ^ {1/5} n ^{-1/5} - h= 0\n",
    "\\tag{3.9}\n",
    "$$\n",
    "\n",
    "The Sheather-Jones plug-in bandwidth $h_{\\text{SJ}}$ is the solution to this equation, where:\n",
    " \n",
    "* $\\kappa$ is the kernel, usually a standard Gaussian density.  \n",
    "* $\\mu_2(\\kappa)$ is the second moment of the distribution $\\kappa$. In the case of a standard Gaussian kernel it is 1.  \n",
    "* $\\hat{S}(g)$ is a kernel based estimate of $R(f'')$  \n",
    "\n",
    "$$ \n",
    "\\hat{S}(g) = [N(N-1)]^{-1} g^{-5} \\sum_{i=1}^N \\sum_{j=1}^N { \\phi^{\\text{iv}}[g^{-1}(X_i - X_j)] }\n",
    "\\tag{3.10}\n",
    "$$\n",
    "\n",
    "$$\n",
    "g(h) = 1.357 [\\hat{S}(a) / \\hat{T}(b) ]^{1/7}  h^{5/7}\n",
    "\\tag{3.11}\n",
    "$$\n",
    "\n",
    "where the term in brackets is the second stage estimate of $R(f'') / R(f''')$.\n",
    "\n",
    "$$\n",
    "\\hat{T}(b) = -[N(N-1)]^{-1} b^{-7} \\sum_{i=1}^N \\sum_{j=1}^N { \\phi^{\\text{vi}}[b^{-1}(X_i - X_j)] }\n",
    "\\tag{3.12}\n",
    "$$\n",
    "\n",
    "For this estimate the bandwidths $a$ and $b$ are \n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "    a = 0.920 \\hat{\\lambda} N ^{-1/7} & \\text{and} & b = 0.912 \\hat{\\lambda} N ^ {-1/9}\n",
    "\\end{array}\n",
    "\\tag{3.13}\n",
    "$$\n",
    "\n",
    "Finally, $\\phi^{\\text{iv}}$ and $\\phi^{\\text{vi}}$ refer to the fourth and sixth derivatives of a standard Gaussian density. A better developed mathematical justification of each of the components can be found in the original paper.\n",
    "\n",
    "One of the advantages of this method over the one by Park and Marron is that the function it minimizes is simpler. In addition, the convergence rate is in the order of $n ^ {-5/14}$, which is better compared to LSCV, BCV and Park and Marron's proposal.\n",
    "\n",
    "One drawback mentioned by Sheather and Jones (1991) and Sheather in *Density estimation* (2004) is that the Sheather-Jones method is derived under the assumption that $f$ is a smooth density. Jones, Marron and Sheather (1996) found that when the true density is easy to estimate, $h_{\\text{SJ}}$ tends to be centered at $h_{\\text{AMISE}}$, while when the true density is harder, $h_{\\text{SJ}}$ is usually centered at a higher value than $h_{\\text{AMISE}}$, meaning that $h_{\\text{SJ}}$ oversmooths.\n",
    "\n",
    "<!-- https://math.stackexchange.com/questions/2287895/are-derivatives-of-the-standard-gaussian-density-bounded -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "\n",
    "# The n-th derivative of a gaussian pdf is given by the pdf times \n",
    "# the n-th probabilists' Hermite polynomial.\n",
    "# ref: https://math.stackexchange.com/questions/2287895/are-derivatives-of-the-standard-gaussian-density-bounded\n",
    "def phi6(x):\n",
    "    return (x ** 6 - 15 * x ** 4 + 45 * x ** 2 - 15) * stats.norm.pdf(x)\n",
    "\n",
    "def phi4(x):\n",
    "    return (x ** 4 - 6 * x ** 2 + 3) * stats.norm.pdf(x)\n",
    "\n",
    "def sj_helper(h, s_a, t_b, x_len, x_len_mult, x_pairwise_diff):\n",
    "    \"\"\"\n",
    "    Equation 12 of Sheather and Jones [1]\n",
    "    Equation 3.9 in this work.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    .. [1] A reliable data-based bandwidth selection method for kernel\n",
    "        density estimation. Simon J. Sheather and Michael C. Jones.\n",
    "        Journal of the Royal Statistical Society, Series B. 1991\n",
    "    \"\"\"\n",
    "    \n",
    "    numerator = 0.375 * np.pi ** -0.5  \n",
    "    g_h = 1.357 * np.abs(s_a / t_b) ** (1 / 7) * h ** (5 / 7)\n",
    "    s_g = np.sum(np.sum(phi4(x_pairwise_diff / g_h), 0))\n",
    "    s_g *= x_len_mult * g_h ** -5\n",
    "    output = (numerator / np.abs(s_g * x_len)) ** 0.2 - h\n",
    "\n",
    "    return output\n",
    "\n",
    "def h_sj(x):\n",
    "    \"\"\"\n",
    "    Computes Sheather-Jones bandwidth for Gaussian KDE\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 1-D array-like\n",
    "        1 dimensional array of sample data from the variable for which a \n",
    "        density estimate is desired.  \n",
    "    Returns\n",
    "    -------\n",
    "    h : float\n",
    "        Bandwidth estimated via Least Squares Cross-Validation\n",
    "    \"\"\"\n",
    "    \n",
    "    x_len = len(x)\n",
    "    x_std = np.std(x)\n",
    "    x_iqr = stats.iqr(x)\n",
    "    \n",
    "    a = 0.92 * x_iqr * x_len ** (-1 / 7)\n",
    "    b = 0.912 * x_iqr * x_len ** (-1 / 9) \n",
    "    \n",
    "    x_len_mult = 1 / (x_len * (x_len - 1))\n",
    "    x_matrix = np.tile(x, (x_len, 1))\n",
    "    x_pairwise_diff = x_matrix - x_matrix.T\n",
    "\n",
    "    t_b = np.sum(np.sum(phi6(x_pairwise_diff / b), 0))\n",
    "    t_b *= - x_len_mult * b ** -7\n",
    "\n",
    "    s_a = np.sum(np.sum(phi4(x_pairwise_diff / a), 0))\n",
    "    s_a *= x_len_mult * a ** -5\n",
    "    \n",
    "    # Silverman's rule as initial value for h\n",
    "    h0 = 1.06 * x_std * x_len ** (-0.2)\n",
    "    \n",
    "    result = fsolve(sj_helper, h0, args=(s_a, t_b, x_len, x_len_mult, x_pairwise_diff))\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "colors = [DARK_BLUE, NIGHT_BLUE, \"black\"]\n",
    "labels = [\"True distribution\", r\"Gaussian KDE with $h_{\\mathrm{SJ}}$\", \"Gaussian KDE with $h_{\\mathrm{Silverman}}$\"]\n",
    "lines = [Line2D([0], [0], color=c, linewidth=4) for c in colors]\n",
    "\n",
    "x_grid = np.linspace(-3.5, 3.5, 500)\n",
    "plt.plot(x_grid, stats.norm.pdf(x_grid), linewidth=4, color=DARK_BLUE)\n",
    "\n",
    "for i in range(20):\n",
    "    rvs = np.random.normal(size=1000)\n",
    "    hsj = h_sj(rvs)\n",
    "    x_kde, y_kde = convolution_kde(rvs, h=hsj)\n",
    "    plt.plot(x_kde, y_kde, linewidth=2, color=NIGHT_BLUE, alpha=0.3)\n",
    "\n",
    "x_kde, y_kde = convolution_kde(rvs)\n",
    "plt.plot(x_kde, y_kde, linewidth=3, linestyle=\"--\", color=\"black\")   \n",
    "plt.legend(lines, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A drawback of the current implementation is that it stores the ${N}\\choose{2}$ pairwise differences between the $N$ sample points. An element-wise computation of each of the terms in the distance matrix that is discarded after contributing to the sum would not require to store all the information at the same time but will be more demanding in terms of computation. It could be implemented in the future if more tests with the method are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Improved Sheather-Jones\n",
    "\n",
    "This section is based in the two following works:\n",
    "\n",
    "* Kernel density estimation via diffusin by Z.I. Botev, J.F. Grotowski and D.P. Kroese (2011)\n",
    "* Chapter 8 of Handbook of Monte Carlo Methods by D.P. Kroese, T. Taimre and Z.I. Botev (2011)\n",
    "\n",
    "The work by Botev et.al. (2011) starts with a strong critic at a conceptual level against normal reference rules to chose bandwidths. Here they introduce a new plug-in method that is *genuinely* non-parametric and completely data-driven. Unlike Sheather and Jones, they do not make any normality assumptions in any of the estimation stages. Furthermore, another advantage of the method is that it does not rely on any numerical optimization method to obtain the result. The authors state their method is not much slower than the computation of a Gaussian rule of thumb (we'll see). Finally, since the authors borrowed ideas from Sheather and Jones, they call their method improved Sheather-Jones.\n",
    "\n",
    "Before we jump to the derivation of the method we should write again some formulas that are goin to be referenced later.\n",
    "\n",
    "Without loss of generality we can consider the data to be on the unit interval $[0, 1]$. If the raw data is not on the unit interval, then it can be rescaled with a linear transformation. \n",
    "\n",
    "First, recall that away from the boudaries the Gaussian kernel $\\varphi_h(x) = \\frac{1}{\\sqrt{2\\pi}h}e^{-\\frac{1}{2}\\left(\\frac{x - X_i}{h}\\right)^2}$ is approximated by the theta function\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "    \\theta(x, X_i; t) = \\sum_{k = - \\infty}^{\\infty}{ \\varphi(x, 2k + X_i; t) +  \\varphi(x, 2k - X_i; t)} & x \\in (0,1)\n",
    "\\end{array}\n",
    "\\tag{3.14}\n",
    "$$\n",
    "\n",
    "and this derived in an approximation to the Gaussian kernel density estimate on the unit interval $(0, 1)$ by the truncated Fourier series expansion\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "  \\displaystyle \\hat{f}(x; t) \\approx \\sum_{k=0}^{m-1}{a_k e^{-k^2\\pi^2 t / 2} \\cos(k\\pi x)}, & N \\gg 1,\n",
    "\\end{array}\n",
    "\\tag{3.15}\n",
    "$$\n",
    "\n",
    "where the coefficients $\\{a_k\\}_{k=0}^{m - 1}$ are given by the cosine transform of the empirical data\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "  a_0 = 1 & \\displaystyle a_k = \\frac{2}{N}\\sum_{i=1}^{N}{\\cos(k\\pi X_i)}, & k = 1, 2, 3, \\cdots .\n",
    "\\end{array}\n",
    "\\tag{3.16}\n",
    "$$\n",
    "\n",
    "And finally, let $t = h ^ 2$ so our notation is consistent with the derivation in the original work.\n",
    "\n",
    "As previously mentioned, plug-in methods replace $R(f'')$ with an estimate. And this is how the story of the estimator of the section starts. \n",
    "\n",
    "Before proposing an estimator $R(f'') = \\lVert f''\\rVert^2$ the authors note that the identity $\\lVert f^{(j)}\\rVert^2 = (-1)^j\\mathbb{E}_f\\left[ f^{(2j)}(X)\\right], \\ j \\ge1$, where $f^{(j)}$ represents the j-th derivative of $f$, suggests two possible plug-in estimators. The first one is \n",
    "\n",
    "$$\n",
    " \\begin{split}\n",
    "  (-1)^j\\widehat{\\mathbb{E}_{f}f^{(2j)}} & \\overset{\\text{def}}{=} \\frac{(-1)^j}{N}\\sum_{k=1}^N{\\hat{f}^{(2j)}(X_k;t_j)}  \\\\\n",
    "   & =  \\displaystyle\\frac{(-1)^j}{N^2}\\sum_{k=1}^{N}{\\sum_{m=1}^{N}{\\varphi^{(2j)}(X_k, X_m; t_j)}}\n",
    "\\end{split}\n",
    "\\tag{3.17}\n",
    "$$\n",
    "\n",
    "where $\\hat{f}$ is the Gaussian KDE with bandwith $\\sqrt{t_j}$ different from the one $\\sqrt{t^*}$ resulting from formula (3.2). Just think in the Sheather-Jones estimator, it is the same logic. In the previous section it was called $g(h)$.\n",
    "\n",
    "The second estimator is\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "  \\widehat{\\lVert f^{(j)} \\rVert^2} & \\overset{\\text{def}}{=} \\lVert \\hat{f}^{(j)}(\\cdot;t_j) \\rVert^2  \\\\\n",
    "   & = \\displaystyle \\frac{1}{N^2}\\sum_{k=1}^{N}{\\sum_{m=1}^{N}{\\int_{\\mathbb{R}}\\varphi^{(j)}(x, X_k;t_j)\n",
    "       \\varphi^{(j)}(x, X_m;t_j) \\text{dx}}} \\\\\n",
    "   & = \\displaystyle\\frac{(-1)^j}{N^2}\\sum_{k=1}^{N}{\\sum_{m=1}^{N}{\\varphi^{(2j)}(X_k, X_m; 2t_j)}}\n",
    "\\end{split}\n",
    "\\tag{3.18}\n",
    "$$\n",
    "\n",
    "For a given bandwidth, both estimators $(-1)^j\\widehat{\\mathbb{E}_{f}f^{(2j)}}$ and $\\widehat{\\lVert f^{(j)} \\rVert^2}$ aim to estimate the same quantity $\\lVert f^{(j)} \\rVert^2$.  \n",
    "The logic the authors use to derive an estimator is to select $t_j$ so that both estimators are aymptotically equivalent in the mean square error sense. In other words, $ \\hat{t}_j $ is chosen so that both $(-1)^j\\widehat{\\mathbb{E}_{f}f^{(2j)}}$ and $\\widehat{\\lVert f^{(j)} \\rVert^2}$ have equal asymptotic mean square error. \n",
    "\n",
    "In pages 15 and 16 of Botev et.al. (2011) it is demonstrated that the estimator that satisfies the requirement is\n",
    "\n",
    "$$\n",
    "\\hat{t}_j = \\left(\\frac{1 + \\frac{1}{2^{j+1/2}}}{3} \\frac{1\\times3\\times5\\times\\cdots\\times(2j-1)}{N\\sqrt{\\pi/2} \\lVert\\hat{f}^{(j+1)}\\rVert^2}\\right)^\\frac{2}{3+2j}\n",
    "\\tag{3.19}\n",
    "$$\n",
    "\n",
    "Next, we can see there is a problem: the computation of $\\lVert\\hat{f}^{(j+1)}\\rVert^2$ requires knowing $\\hat{t}_{j+1}$, which also requires having estimated $\\hat{t}_{j+2}$, and so on, as one can appreciate from formulas (3.18) and (3.19). We face the problem of computing the infinite sequence $\\{ \\hat{t}_{j+k},\\ k \\ge 1\\}$. \n",
    "\n",
    "Nevertheless, given $\\hat{t}_{l+1}$ for some integer $l$, we can compute all the $\\{\\hat{t}_j, 1\\le j \\le l \\}$ in a recursive fashion, and then estimate $t^*$ from (3.2). This motivates the **l-stage direct plug-in bandwidth selector** defined as follows:\n",
    "\n",
    "**Algorrithm (l-stage Direct Plug-in Bandwidth Selector)**  \n",
    "Given an integer $l \\ge 3$, execute the following steps:  \n",
    "\n",
    "1. Compute $\\lVert\\hat{f}^{(l+2)}\\rVert^2$ by assuming that $f$ is a **Gaussian density** with mean and variance estimated with the iid sample $X_1, \\cdots, X_N$. Here we can think that $j=l+1$, then $j+1=l+2$.\n",
    "2. Using $\\lVert\\hat{f}^{(j+1)}\\rVert^2$ compute $\\hat{t}_j$ via (3.19).\n",
    "3. Using $\\hat{t}_j$ compute $\\lVert\\hat{f}^{(j)}\\rVert^2$ via (3.18).\n",
    "4. If $j \\gt2$, $j=j-1$ repreat from Step 2; otherwise, continue with Step 5.\n",
    "5. Use $\\hat{t}_2$ to compute $\\lVert\\hat{f}^{(2)}\\rVert^2$ and obtain $\\hat{t^*}$ from (3.2).\n",
    "\n",
    "Then, the **l-stage direct plug-in bandwidth selector** requires the estimation of $\\lVert\\hat{f}^{(j)}\\rVert^2$, with $2 \\le j \\le l+1$ via the plug-in estimator (3.18).  \n",
    "\n",
    "To improve the method the procedure is described in a slightly more abstract way as follows.\n",
    "\n",
    "Let's represent the functional dependency of $\\hat{t}_j$ on $\\hat{t}_{j+1}$ in (3.19) as:\n",
    "\n",
    "$$\n",
    "\\hat{t}_j = \\gamma_j(\\hat{t}_{j+1})\n",
    "\\tag{3.20}\n",
    "$$\n",
    "\n",
    "It is clear that $\\hat{t}_j = \\gamma_j(\\gamma_{j+1}(\\hat{t}_{j+2})) = \\gamma_j(\\gamma_{j+1}(\\gamma_{j+2}(\\hat{t}_{j+2}))) = \\cdots$. Thus, the composition could be written in a simpler way as follows:\n",
    "\n",
    "$$\n",
    "\\gamma^{[k]}(t)= \\underbrace{\\gamma_1(\\cdots\\gamma_{k-1}(\\gamma_k)}_{k\\ \\text{times}}(t))\\cdots), \\ \\ k\\ge1.\n",
    "\\tag{3.21}\n",
    "$$\n",
    "\n",
    "Observing formulas (3.19) y (3.12) we note the estimator of $t^*$ satisfies:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c c}\n",
    "\\hat{t^*} = \\zeta\\hat{t}_1 = \\zeta\\gamma^{[1]}(\\hat{t}_2) = \\zeta\\gamma^{[2]}(\\hat{t}_3) = \\cdots = \\zeta\\gamma^{[l]}(\\hat{t}_{l+1}), & \\displaystyle \\zeta = \\left(\\frac{6\\sqrt{2} - 3}{7} \\right)^{2/5} \\approx 0.907\n",
    "\\end{array}\n",
    "\\tag{3.22}\n",
    "$$\n",
    "\n",
    "Then, for any given integer $l>0$, the bandwidth selection via the direct plug-in method consists in the computation of:\n",
    "\n",
    "$$\n",
    "\\hat{t^*} = \\zeta\\gamma^{[l]}(\\hat{t}_{l+1})\n",
    "\\tag{3.23}\n",
    "$$\n",
    "\n",
    "where $\\hat{t}_{l+1}$ is calculated via (3.19) assuming that $f$ is $\\lVert\\hat{f}^{(l+2)}\\rVert^2$ the density of a Gaussian distribution with mean and variance estimated from the sample.\n",
    "\n",
    "The **weakest point** of this procedure is that it **assumes the true $f$ is Gaussian** in order to compute  $\\lVert\\hat{f}^{(l+2)}\\rVert^2$. This assumption can lead us to poor estimations of $t^*$, specially when the true distribution differs substantially from a normal.\n",
    "\n",
    "Instead, the **improved plug-in** or **improved Sheather-Jones** method is proposed. It generally gives a *typically unique* solution to the following non-linear equation:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "  t = \\zeta\\gamma^{[l]}(t), & \\text{for large enough}\\ l\n",
    "\\end{array}\n",
    "\\tag{3.24}\n",
    "$$\n",
    "\n",
    "as the near-optimal bandwidth estimate. This method is **free of any normality assumptions** and consequently it is more reliable and accurate than the majority of the alternative plug-in methods.\n",
    "\n",
    "Now we present the implementation details of this algorithm. Without loss of generality, it is assumed the data lies in the interval $[0, 1]$ and we use the theta function approximation. \n",
    "\n",
    "The data is binned in a grid of size $n$ and the coefficients $\\{a_k\\}_{k=0}^{n-1}$ in the truncated Fourier series expansion are computed using the fast cosine transform. Then, for a given t,\n",
    "\n",
    "$$\n",
    "\\lVert\\hat{f}^{(j)}\\rVert^2 \\approx \\frac{\\pi^{2j}}{2}\\sum_{k=1}^{n-1}{k^{2j}a_k^2\\text{e}^{-k^2\\pi^2t}}\n",
    "\\tag{3.25}\n",
    "$$\n",
    "\n",
    "Thus, computation of the right-hand side of (3.24) is an $O(n)$ operation.  \n",
    "\n",
    "Next, we present the implementation of this method in Python. In addition, $l=7$ is used. Botev et.al. (2011) suggest that larger values of $l$ do not change the value of the root in (3.24) in a substantial way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from my_functions import *\n",
    "from scipy.optimize import fsolve\n",
    "from scipy.optimize import fminbound\n",
    "\n",
    "def _fixed_point(t, N, k_sq, a_sq):\n",
    "\n",
    "    # To avoid prevent powers from overflowing.\n",
    "    k_sq = np.asfarray(k_sq, dtype='float')\n",
    "    a_sq = np.asfarray(a_sq, dtype='float')\n",
    "\n",
    "    l = 7\n",
    "    f = 0.5 * np.pi ** (2.0 * l) * sum(k_sq ** l * a_sq * np.exp(-k_sq * np.pi ** 2.0 * t))\n",
    "\n",
    "    for j in reversed(range(2, l)):\n",
    "        c1  = (1 + 0.5**(j + 0.5)) / 3.0\n",
    "        c2  = np.product(np.arange(1., 2. * j + 1., 2., dtype = 'float')) / np.sqrt(np.pi / 2)\n",
    "        t_j = np.power((c1 * c2 / (N * f)), (2 / (3 + 2 * j)))\n",
    "        f   = 0.5 * np.pi ** (2. * j) * sum(k_sq ** j * a_sq * np.exp(-k_sq * np.pi ** 2. * t_j) )\n",
    "\n",
    "    out = t - (2. * N * np.sqrt(np.pi) * f) ** (-0.4)\n",
    "    return out\n",
    "\n",
    "def h_isj(x, grid_len = 256):\n",
    "    \n",
    "    x_len = len(x)\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    x_range = x_max - x_min\n",
    "    x_std = np.std(x)\n",
    "    \n",
    "    grid_min = x_min - 0.5 * x_std\n",
    "    grid_max = x_max + 0.5 * x_std\n",
    "       \n",
    "    # Relative frequency per bin\n",
    "    f, edges = np.histogram(x, bins=grid_len, range=(grid_min, grid_max))\n",
    "    f = f / x_len\n",
    "\n",
    "    # Discrete cosine transform of the data\n",
    "    a_k = dct1d(f)\n",
    "    \n",
    "    k_sq = np.arange(1, grid_len) ** 2\n",
    "    a_sq = a_k[range(1, grid_len)] ** 2\n",
    "    \n",
    "    t = fsolve(_fixed_point, 0.02, args=(x_len, k_sq, a_sq))\n",
    "    h = np.sqrt(t[0]) * x_range\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "colors = [DARK_BLUE, NIGHT_BLUE, \"black\"]\n",
    "labels = [\"True distribution\", r\"Gaussian KDE with $h_{\\mathrm{ISJ}}$\", \"Gaussian KDE with $h_{\\mathrm{Silverman}}$\"]\n",
    "lines = [Line2D([0], [0], color=c, linewidth=4) for c in colors]\n",
    "\n",
    "x_grid = np.linspace(-3.5, 3.5, 500)\n",
    "plt.plot(x_grid, stats.norm.pdf(x_grid), linewidth=4, color=DARK_BLUE)\n",
    "\n",
    "for i in range(20):\n",
    "    rvs = np.random.normal(size=1000)\n",
    "    hisj = h_isj(rvs)\n",
    "    x_kde, y_kde = convolution_kde(rvs, h=hisj)\n",
    "    plt.plot(x_kde, y_kde, linewidth=2, color=NIGHT_BLUE, alpha=0.3)\n",
    "\n",
    "x_kde, y_kde = convolution_kde(rvs)\n",
    "plt.plot(x_kde, y_kde, linewidth=3, linestyle=\"--\", color=\"black\")   \n",
    "plt.legend(lines, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to the original problem..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a mostrar el ejemplo motivacional de arriba (como se comportan SJ e ISJ) y tambien vamos a mostrar el ejemplo que usan Botev et. al en el paper. Mixture of gaussians with variance 1 and means of -30 and 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rvs(n, mean, sd):\n",
    "    assert len(mean) == len(sd)\n",
    "    x = np.concatenate((\n",
    "        list(map(lambda m, s: stats.norm.rvs(m, s, int(n / len(mean))), mean, sd))\n",
    "    ))\n",
    "    return x\n",
    "\n",
    "def get_pdf(x_grid, mean, sd):\n",
    "    assert len(mean) == len(sd)\n",
    "    pdf = (1 / len(mean)) * np.sum(list((map(lambda m, s: stats.norm.pdf(x_grid, m, s), mean, sd))), axis=0)\n",
    "    return pdf\n",
    "\n",
    "np.random.seed(1234)\n",
    "x_grid = np.linspace(-6, 7, 500)\n",
    "mean = [-3, -1.8, 3]\n",
    "sd = [0.3, 0.45, 2]\n",
    "n = 1500\n",
    "reps = 20\n",
    "\n",
    "colors = [DARK_BLUE, NIGHT_BLUE]\n",
    "lines = [Line2D([0], [0], color=c, linewidth=4) for c in colors]\n",
    "labels = [\"True distribution\", \"Estimation\"]\n",
    "\n",
    "pdf_true = get_pdf(x_grid, mean, sd)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize = (15, 15))\n",
    "\n",
    "for i, j in zip([0, 0, 1, 1], [0, 1, 0, 1]):\n",
    "    axes[i, j].plot(x_grid, pdf_true, linewidth=3, linestyle=\"--\", color=\"black\")\n",
    "\n",
    "for i in range(reps):\n",
    "    rvs = generate_rvs(n, mean, sd)\n",
    "    \n",
    "    hlscv = h_cv(rvs)\n",
    "    hsj = h_sj(rvs)\n",
    "    hisj = h_isj(rvs)\n",
    "    \n",
    "    x_sil, y_sil = convolution_kde(rvs)\n",
    "    x_lscv, y_lscv = convolution_kde(rvs, h=hlscv)\n",
    "    x_sj, y_sj = convolution_kde(rvs, h=hsj)\n",
    "    x_isj, y_isj = convolution_kde(rvs, h=hisj)\n",
    "    \n",
    "    axes[0, 0].plot(x_sil, y_sil, linewidth=2, color=NIGHT_BLUE, alpha=0.5)\n",
    "    axes[0, 1].plot(x_sil, y_lscv, linewidth=2, color=NIGHT_BLUE, alpha=0.5)\n",
    "    axes[1, 0].plot(x_sil, y_sj, linewidth=2, color=NIGHT_BLUE, alpha=0.5)\n",
    "    axes[1, 1].plot(x_sil, y_isj, linewidth=2, color=NIGHT_BLUE, alpha=0.5)\n",
    "\n",
    "for i, j in zip([0, 0, 1, 1], [0, 1, 0, 1]):\n",
    "    axes[i, j].hist(rvs, density=True, bins=40, alpha=0.7, Color=BLUE)\n",
    "\n",
    "axes[i, j].set_title('Sample size: ' + str(n))\n",
    "axes[0, 0].set_title(\"Silverman's rule\")\n",
    "axes[0, 1].set_title(\"Least Squares Cross-Validation\")\n",
    "axes[1, 0].set_title(\"Sheather Jones\")\n",
    "axes[1, 1].set_title(\"Improved Sheather Jones\")    \n",
    "\n",
    "# plt.hist(rvs, density=True, bins=40, alpha=0.7, Color=BLUE)\n",
    "# plt.xlim(-6, 9)\n",
    "\n",
    "# plt.legend(lines, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
